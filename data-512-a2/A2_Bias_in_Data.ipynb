{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias in Wikipedia hostile speech analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to identify potential bias in human-annotated data and describe implications of those biases. <br>\n",
    "The goal is to analyze multiple datasets of hostile speech used by Wikimedia Research and Jigsaw for labelling and uncover potential biases in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Import packages and datasets](#Import-packages-and-datasets)\n",
    "2. [Exploratory data analysis](#Exploratory-data-analysis)\n",
    "3. [Analysis #1: Words most associated with hostile speech](#Analysis-1:-words-most-associated-with-hostile-speech)\n",
    "   1. [Research question](#Research-question:)\n",
    "   2. [Generate toxicity label](#Generate-binary-toxicity-label)\n",
    "   3. [Clean the comment column](#Clean-the-comment-column)\n",
    "   4. [Join label with comments](#Join-toxicity-label-and-comments)\n",
    "   5. [Rerun analysis for the aggressive posts](#Rerun-the-analysis-for-aggressive-comments)\n",
    "   5. [Count the most frequent word in toxic posts using sklearn's CountVectorizer](#Apply-sklearn's-CountVectorizer-to-get-a-bag-of-token-words-and-a-matrix-of-token-counts)\n",
    "   5. [Observations for toxic comments](#Observations-of-toxic-comments)\n",
    "   6. [Rerun the analysis on aggressive comments](#Rerun-the-analysis-for-aggressive-comments)\n",
    "   7. [Observations for aggressive comments](#Observations-of-aggressive-comments)\n",
    "   8. [Implications of analysis 1](#Implications-of-analysis-1)\n",
    "   \n",
    "4. [Analysis #2: Demographic of crowdsourcing workers](#Analysis-2:-demographic-information-from-the-crowdsourcing-workers-for-toxicity-analysis)\n",
    "   1. [Research question](#Research-question:-Are-the-annotators-truly-representative-of-the-general-public?)\n",
    "   2. [Check number of workers](#Check-number-of-unique-workers)\n",
    "   3. [Check the distribution of demographic info](#Check-the-distribution-of-demographic-columns)\n",
    "   4. [Observations of demographic analysis](#Observations-of-demographic-analysis)\n",
    "   5. [Implications of demographic analysis](#Implications-of-demographic-analysis)\n",
    "5. [Discuss further implications](#Discuss-further-implications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from functools import reduce \n",
    "pd.set_option('display.max_rows', 100)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOT_PATH = os.getcwd()\n",
    "DATA_PATH = os.path.join(ROOT_PATH, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxicity_annotated_comments.tsv',\n",
       " 'aggression_annotations.tsv',\n",
       " 'toxicity_annotations.tsv',\n",
       " 'aggression_annotated_comments.tsv',\n",
       " 'toxicity_worker_demographics.tsv',\n",
       " 'aggression_worker_demographics.tsv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toxic_comments = pd.read_csv(os.path.join(DATA_PATH, 'toxicity_annotated_comments.tsv'), sep = '\\t')\n",
    "toxic_annotations = pd.read_csv(os.path.join(DATA_PATH, 'toxicity_annotations.tsv'), sep = '\\t')\n",
    "toxic_demographics = pd.read_csv(os.path.join(DATA_PATH, 'toxicity_worker_demographics.tsv'), sep = '\\t')\n",
    "\n",
    "agg_comments = pd.read_csv(os.path.join(DATA_PATH, 'aggression_annotated_comments.tsv'), sep = '\\t')\n",
    "agg_annotations = pd.read_csv(os.path.join(DATA_PATH, 'aggression_annotations.tsv'), sep = '\\t')\n",
    "agg_demographics = pd.read_csv(os.path.join(DATA_PATH, 'aggression_worker_demographics.tsv'), sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check what variables are included in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>This:NEWLINE_TOKEN:One can make an analogy in ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216.0</td>\n",
       "      <td>`NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>user</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8953.0</td>\n",
       "      <td>Elected or Electoral? JHK</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547.0</td>\n",
       "      <td>`This is such a fun entry.   DevotchkaNEWLINE_...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28959.0</td>\n",
       "      <td>Please relate the ozone hole to increases in c...</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rev_id                                            comment  year  \\\n",
       "0   2232.0  This:NEWLINE_TOKEN:One can make an analogy in ...  2002   \n",
       "1   4216.0  `NEWLINE_TOKENNEWLINE_TOKEN:Clarification for ...  2002   \n",
       "2   8953.0                          Elected or Electoral? JHK  2002   \n",
       "3  26547.0  `This is such a fun entry.   DevotchkaNEWLINE_...  2002   \n",
       "4  28959.0  Please relate the ozone hole to increases in c...  2002   \n",
       "\n",
       "   logged_in       ns  sample  split  \n",
       "0       True  article  random  train  \n",
       "1       True     user  random  train  \n",
       "2      False  article  random   test  \n",
       "3       True  article  random  train  \n",
       "4       True  article  random   test  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>worker_id</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>toxicity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>723</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>4000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>3989</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>3341</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>1574</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rev_id  worker_id  toxicity  toxicity_score\n",
       "0  2232.0        723         0             0.0\n",
       "1  2232.0       4000         0             0.0\n",
       "2  2232.0       3989         0             1.0\n",
       "3  2232.0       3341         0             0.0\n",
       "4  2232.0       1574         0             1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_annotations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>english_first_language</th>\n",
       "      <th>age_group</th>\n",
       "      <th>education</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>18-30</td>\n",
       "      <td>bachelors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1617</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>45-60</td>\n",
       "      <td>bachelors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1394</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bachelors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>311</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>30-45</td>\n",
       "      <td>bachelors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1980</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>45-60</td>\n",
       "      <td>masters</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   worker_id  gender  english_first_language age_group  education\n",
       "0         85  female                       0     18-30  bachelors\n",
       "1       1617  female                       0     45-60  bachelors\n",
       "2       1394  female                       0       NaN  bachelors\n",
       "3        311    male                       0     30-45  bachelors\n",
       "4       1980    male                       0     45-60    masters"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \tAnalysis 1: words most associated with hostile speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research question: \n",
    "- What are the most common words associated with comments labelled as hostile speech? \n",
    "- Are there any differences between the most frequent words for different types of hostile speech (toxicity and aggresive)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate binary toxicity label\n",
    "#### Check the distribution of the toxicity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    159686.000000\n",
       "mean          0.145049\n",
       "std           0.253866\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           0.000000\n",
       "75%           0.200000\n",
       "max           1.000000\n",
       "Name: toxicity, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_annotations.groupby('rev_id')['toxicity'].mean().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use 0.5 as the threshold for denoting a comment as toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label(df, key_column, label_column, new_name):\n",
    "    avg_score = df.groupby(key_column)[label_column].mean().reset_index(name = new_name)\n",
    "    avg_score[\"\".join([new_name, \"_bool\"])] = (avg_score[new_name] > 0.5) * 1\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rev_id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>toxic_bool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2232.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4216.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8953.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26547.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28959.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159681</th>\n",
       "      <td>699848324.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159682</th>\n",
       "      <td>699851288.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159683</th>\n",
       "      <td>699857133.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159684</th>\n",
       "      <td>699891012.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159685</th>\n",
       "      <td>699897151.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159686 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             rev_id  toxic  toxic_bool\n",
       "0            2232.0    0.1           0\n",
       "1            4216.0    0.0           0\n",
       "2            8953.0    0.0           0\n",
       "3           26547.0    0.0           0\n",
       "4           28959.0    0.2           0\n",
       "...             ...    ...         ...\n",
       "159681  699848324.0    0.0           0\n",
       "159682  699851288.0    0.0           0\n",
       "159683  699857133.0    0.0           0\n",
       "159684  699891012.0    0.4           0\n",
       "159685  699897151.0    0.0           0\n",
       "\n",
       "[159686 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_toxicity_score = generate_label(toxic_annotations, 'rev_id', 'toxicity', 'toxic')\n",
    "avg_toxicity_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the comment column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove newline, tab tokens, and special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_comment_col(df):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['comment'] = df_copy['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "    df_copy['comment'] = df_copy['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "    df_copy['comment'] = df_copy['comment'].apply(lambda x: re.sub('[^A-Za-z]+', ' ', x))\n",
    "    \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "toxic_comments_rm_special_char = clean_comment_col(toxic_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join toxicity label and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def join_label_with_text(label_df, comment_df, key_column, bool_column):\n",
    "    combined_df = comment_df.join(label_df[[key_column, bool_column]].set_index(key_column), on = key_column)\n",
    "    \n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_with_label = join_label_with_text(avg_toxicity_score, toxic_comments_rm_special_char, 'rev_id', 'toxic_bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply sklearn's CountVectorizer to get a bag of token words and a matrix of token counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract rows with label toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_target_hostile_column(combined_df, bool_column):\n",
    "    return combined_df[combined_df[bool_column] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15362, 8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_comment = extract_target_hostile_column(comment_with_label, 'toxic_bool')\n",
    "toxic_comment.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get tokens and counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tokens_and_counts(combined_df):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(combined_df['comment'])\n",
    "    word_token = vectorizer.get_feature_names()\n",
    "    count_matrix = X.toarray()\n",
    "    \n",
    "    return word_token, count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_tokens, counts_matrix = get_tokens_and_counts(toxic_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate frequency of each token and get the most frequent words for toxic comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_freqs_sorted(tokens, matrix):\n",
    "    freq_for_each_token = np.sum(matrix, axis = 0)\n",
    "    token_tuples = list(zip(tokens,freq_for_each_token))\n",
    "    word_freq_df = pd.DataFrame(token_tuples, columns=['Word','frequency'])\\\n",
    "                    .sort_values(by = 'frequency', ascending=False)\n",
    "    word_freq_df['rank'] = list(range(1, len(tokens)+1))\n",
    "    return word_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_freq_df = words_freqs_sorted(word_tokens, counts_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31517</th>\n",
       "      <td>you</td>\n",
       "      <td>39353</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27870</th>\n",
       "      <td>the</td>\n",
       "      <td>20718</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>and</td>\n",
       "      <td>15881</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28277</th>\n",
       "      <td>to</td>\n",
       "      <td>15800</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14674</th>\n",
       "      <td>is</td>\n",
       "      <td>12720</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19523</th>\n",
       "      <td>of</td>\n",
       "      <td>11500</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11101</th>\n",
       "      <td>fuck</td>\n",
       "      <td>9959</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31545</th>\n",
       "      <td>your</td>\n",
       "      <td>9171</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>are</td>\n",
       "      <td>8486</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27863</th>\n",
       "      <td>that</td>\n",
       "      <td>8295</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14725</th>\n",
       "      <td>it</td>\n",
       "      <td>8244</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13953</th>\n",
       "      <td>in</td>\n",
       "      <td>6830</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18528</th>\n",
       "      <td>my</td>\n",
       "      <td>6429</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17355</th>\n",
       "      <td>me</td>\n",
       "      <td>5558</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27996</th>\n",
       "      <td>this</td>\n",
       "      <td>5525</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19679</th>\n",
       "      <td>on</td>\n",
       "      <td>4922</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19214</th>\n",
       "      <td>not</td>\n",
       "      <td>4866</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10759</th>\n",
       "      <td>for</td>\n",
       "      <td>4754</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12727</th>\n",
       "      <td>have</td>\n",
       "      <td>4641</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8089</th>\n",
       "      <td>do</td>\n",
       "      <td>4031</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18980</th>\n",
       "      <td>nigger</td>\n",
       "      <td>4015</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31001</th>\n",
       "      <td>with</td>\n",
       "      <td>3952</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30851</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>3708</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25110</th>\n",
       "      <td>shit</td>\n",
       "      <td>3628</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>be</td>\n",
       "      <td>3602</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16309</th>\n",
       "      <td>like</td>\n",
       "      <td>3527</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30580</th>\n",
       "      <td>what</td>\n",
       "      <td>3523</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>all</td>\n",
       "      <td>3489</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26978</th>\n",
       "      <td>suck</td>\n",
       "      <td>3458</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>an</td>\n",
       "      <td>3328</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11137</th>\n",
       "      <td>fucking</td>\n",
       "      <td>3295</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25866</th>\n",
       "      <td>so</td>\n",
       "      <td>3134</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>am</td>\n",
       "      <td>3110</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>as</td>\n",
       "      <td>3039</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4145</th>\n",
       "      <td>can</td>\n",
       "      <td>3025</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>ass</td>\n",
       "      <td>2945</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>go</td>\n",
       "      <td>2877</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30923</th>\n",
       "      <td>will</td>\n",
       "      <td>2841</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916</th>\n",
       "      <td>faggot</td>\n",
       "      <td>2730</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13719</th>\n",
       "      <td>if</td>\n",
       "      <td>2649</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12710</th>\n",
       "      <td>hate</td>\n",
       "      <td>2633</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8178</th>\n",
       "      <td>don</td>\n",
       "      <td>2590</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15243</th>\n",
       "      <td>just</td>\n",
       "      <td>2498</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22688</th>\n",
       "      <td>re</td>\n",
       "      <td>2420</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30338</th>\n",
       "      <td>was</td>\n",
       "      <td>2420</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3948</th>\n",
       "      <td>but</td>\n",
       "      <td>2407</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29443</th>\n",
       "      <td>up</td>\n",
       "      <td>2385</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12766</th>\n",
       "      <td>he</td>\n",
       "      <td>2380</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30668</th>\n",
       "      <td>who</td>\n",
       "      <td>2374</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19083</th>\n",
       "      <td>no</td>\n",
       "      <td>2368</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  frequency  rank\n",
       "31517        you      39353     1\n",
       "27870        the      20718     2\n",
       "1060         and      15881     3\n",
       "28277         to      15800     4\n",
       "14674         is      12720     5\n",
       "19523         of      11500     6\n",
       "11101       fuck       9959     7\n",
       "31545       your       9171     8\n",
       "1464         are       8486     9\n",
       "27863       that       8295    10\n",
       "14725         it       8244    11\n",
       "13953         in       6830    12\n",
       "18528         my       6429    13\n",
       "17355         me       5558    14\n",
       "27996       this       5525    15\n",
       "19679         on       4922    16\n",
       "19214        not       4866    17\n",
       "10759        for       4754    18\n",
       "12727       have       4641    19\n",
       "8089          do       4031    20\n",
       "18980     nigger       4015    21\n",
       "31001       with       3952    22\n",
       "30851  wikipedia       3708    23\n",
       "25110       shit       3628    24\n",
       "2483          be       3602    25\n",
       "16309       like       3527    26\n",
       "30580       what       3523    27\n",
       "796          all       3489    28\n",
       "26978       suck       3458    29\n",
       "1021          an       3328    30\n",
       "11137    fucking       3295    31\n",
       "25866         so       3134    32\n",
       "919           am       3110    33\n",
       "1624          as       3039    34\n",
       "4145         can       3025    35\n",
       "1691         ass       2945    36\n",
       "11841         go       2877    37\n",
       "30923       will       2841    38\n",
       "9916      faggot       2730    39\n",
       "13719         if       2649    40\n",
       "12710       hate       2633    41\n",
       "8178         don       2590    42\n",
       "15243       just       2498    43\n",
       "22688         re       2420    44\n",
       "30338        was       2420    45\n",
       "3948         but       2407    46\n",
       "29443         up       2385    47\n",
       "12766         he       2380    48\n",
       "30668        who       2374    49\n",
       "19083         no       2368    50"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq_df[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations of toxic comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the most common English words, words including 'fuck', 'nigger', 'shit', 'suck', 'fucking', 'ass', 'faggot', 'hate', 'don' appear in the top 50 most common words for comments that have average toxicity annotations over 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun the analysis for aggressive comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    115864.000000\n",
       "mean          0.185828\n",
       "std           0.271089\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           0.100000\n",
       "75%           0.250000\n",
       "max           1.000000\n",
       "Name: aggression, dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_annotations.groupby('rev_id')['aggression'].mean().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate label column\n",
    "avg_aggressive_score = generate_label(agg_annotations, 'rev_id', 'aggression', 'agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean comment column\n",
    "agg_comments_rm_special_char = clean_comment_col(agg_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# join label with comment\n",
    "agg_comment_with_label = join_label_with_text(avg_aggressive_score, agg_comments_rm_special_char, \n",
    "                                              'rev_id', 'agg_bool')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14782, 8)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_comment = extract_target_hostile_column(agg_comment_with_label, 'agg_bool')\n",
    "agg_comment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get tokens and counts\n",
    "word_tokens_agg, counts_matrix_agg = get_tokens_and_counts(agg_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32230</th>\n",
       "      <td>you</td>\n",
       "      <td>49535</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28540</th>\n",
       "      <td>the</td>\n",
       "      <td>23445</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28912</th>\n",
       "      <td>to</td>\n",
       "      <td>18309</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>and</td>\n",
       "      <td>17886</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15050</th>\n",
       "      <td>is</td>\n",
       "      <td>15309</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11374</th>\n",
       "      <td>fuck</td>\n",
       "      <td>14587</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20050</th>\n",
       "      <td>of</td>\n",
       "      <td>13046</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32254</th>\n",
       "      <td>your</td>\n",
       "      <td>10661</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>are</td>\n",
       "      <td>9199</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28531</th>\n",
       "      <td>that</td>\n",
       "      <td>9073</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15111</th>\n",
       "      <td>it</td>\n",
       "      <td>8970</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14290</th>\n",
       "      <td>in</td>\n",
       "      <td>8017</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19017</th>\n",
       "      <td>my</td>\n",
       "      <td>7229</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28653</th>\n",
       "      <td>this</td>\n",
       "      <td>6337</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17827</th>\n",
       "      <td>me</td>\n",
       "      <td>5941</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19718</th>\n",
       "      <td>not</td>\n",
       "      <td>5743</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19477</th>\n",
       "      <td>nigger</td>\n",
       "      <td>5654</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11053</th>\n",
       "      <td>for</td>\n",
       "      <td>5357</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20193</th>\n",
       "      <td>on</td>\n",
       "      <td>5263</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13011</th>\n",
       "      <td>have</td>\n",
       "      <td>5154</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25750</th>\n",
       "      <td>shit</td>\n",
       "      <td>4995</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27635</th>\n",
       "      <td>suck</td>\n",
       "      <td>4962</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8358</th>\n",
       "      <td>do</td>\n",
       "      <td>4460</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1747</th>\n",
       "      <td>ass</td>\n",
       "      <td>4304</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31724</th>\n",
       "      <td>with</td>\n",
       "      <td>4244</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827</th>\n",
       "      <td>all</td>\n",
       "      <td>4204</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>960</th>\n",
       "      <td>am</td>\n",
       "      <td>4158</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>an</td>\n",
       "      <td>3921</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10251</th>\n",
       "      <td>faggot</td>\n",
       "      <td>3911</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2587</th>\n",
       "      <td>be</td>\n",
       "      <td>3874</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31634</th>\n",
       "      <td>will</td>\n",
       "      <td>3755</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12985</th>\n",
       "      <td>hate</td>\n",
       "      <td>3733</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16762</th>\n",
       "      <td>like</td>\n",
       "      <td>3712</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31554</th>\n",
       "      <td>wikipedia</td>\n",
       "      <td>3696</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31281</th>\n",
       "      <td>what</td>\n",
       "      <td>3660</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>as</td>\n",
       "      <td>3502</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11410</th>\n",
       "      <td>fucking</td>\n",
       "      <td>3458</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12126</th>\n",
       "      <td>go</td>\n",
       "      <td>3421</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>can</td>\n",
       "      <td>3373</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10448</th>\n",
       "      <td>fat</td>\n",
       "      <td>3287</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26491</th>\n",
       "      <td>so</td>\n",
       "      <td>3110</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14044</th>\n",
       "      <td>if</td>\n",
       "      <td>3091</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21106</th>\n",
       "      <td>penis</td>\n",
       "      <td>3080</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>die</td>\n",
       "      <td>3048</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8445</th>\n",
       "      <td>don</td>\n",
       "      <td>2847</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23281</th>\n",
       "      <td>re</td>\n",
       "      <td>2796</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6765</th>\n",
       "      <td>cunt</td>\n",
       "      <td>2786</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11755</th>\n",
       "      <td>gay</td>\n",
       "      <td>2752</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30162</th>\n",
       "      <td>up</td>\n",
       "      <td>2687</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19582</th>\n",
       "      <td>no</td>\n",
       "      <td>2627</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Word  frequency  rank\n",
       "32230        you      49535     1\n",
       "28540        the      23445     2\n",
       "28912         to      18309     3\n",
       "1105         and      17886     4\n",
       "15050         is      15309     5\n",
       "11374       fuck      14587     6\n",
       "20050         of      13046     7\n",
       "32254       your      10661     8\n",
       "1507         are       9199     9\n",
       "28531       that       9073    10\n",
       "15111         it       8970    11\n",
       "14290         in       8017    12\n",
       "19017         my       7229    13\n",
       "28653       this       6337    14\n",
       "17827         me       5941    15\n",
       "19718        not       5743    16\n",
       "19477     nigger       5654    17\n",
       "11053        for       5357    18\n",
       "20193         on       5263    19\n",
       "13011       have       5154    20\n",
       "25750       shit       4995    21\n",
       "27635       suck       4962    22\n",
       "8358          do       4460    23\n",
       "1747         ass       4304    24\n",
       "31724       with       4244    25\n",
       "827          all       4204    26\n",
       "960           am       4158    27\n",
       "1060          an       3921    28\n",
       "10251     faggot       3911    29\n",
       "2587          be       3874    30\n",
       "31634       will       3755    31\n",
       "12985       hate       3733    32\n",
       "16762       like       3712    33\n",
       "31554  wikipedia       3696    34\n",
       "31281       what       3660    35\n",
       "1682          as       3502    36\n",
       "11410    fucking       3458    37\n",
       "12126         go       3421    38\n",
       "4286         can       3373    39\n",
       "10448        fat       3287    40\n",
       "26491         so       3110    41\n",
       "14044         if       3091    42\n",
       "21106      penis       3080    43\n",
       "7864         die       3048    44\n",
       "8445         don       2847    45\n",
       "23281         re       2796    46\n",
       "6765        cunt       2786    47\n",
       "11755        gay       2752    48\n",
       "30162         up       2687    49\n",
       "19582         no       2627    50"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq_agg_df = words_freqs_sorted(word_tokens_agg, counts_matrix_agg)\n",
    "word_freq_agg_df[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations of aggressive comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For comments with average aggressive score over 0.5, words including 'fuck', 'nigger', 'shit', 'suck', 'ass', 'faggot', 'hate', 'fucking', 'fat', 'penis', 'die', 'don', 'gay' appear in the top 50 most commonly-associated words. \n",
    "All top 50 commonly-associated words with toxic are present here, and their ranks are higher. There are words such as 'penis', 'fat', 'gay' that are not present in the top 50 of toxic comments and may potentially used as a negative attack. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implications of analysis 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Using ngram for the model might generate many false alarms and not ideal for identifying hostile speech.\n",
    "  - Reasoning: While words like 'fuck', 'nigger', 'shit', and 'hate' can be a clear indicator for hostile speech, it might be present in an article that discusses associated cultural or social issues. Using ngram and using word frequency to train the model ignore the context and the predictions are not fully representitive of the real-world online discussions.\n",
    "- Using LSTM, which takes into account the relationships between words, might be a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis 2: demographic information from the crowdsourcing workers for toxicity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research question: Are the annotators truly representative of the general public? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check number of unique workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3591"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_demographics.worker_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the distribution of demographic columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender\n",
      "male      2327\n",
      "female    1263\n",
      "other        1\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "english_first_language\n",
      "0                         2925\n",
      "1                          666\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "age_group\n",
      "18-30        1862\n",
      "30-45        1247\n",
      "45-60         296\n",
      "Under 18       79\n",
      "Over 60        30\n",
      "dtype: int64\n",
      "\n",
      "\n",
      "education   \n",
      "bachelors       1441\n",
      "hs              1026\n",
      "masters          546\n",
      "professional     441\n",
      "some              93\n",
      "doctorate         41\n",
      "none               3\n",
      "dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_columns = list(toxic_demographics.columns)\n",
    "demo_columns.remove('worker_id')\n",
    "for column in demo_columns:\n",
    "    print(toxic_demographics[[column]].value_counts())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender  age_group\n",
       "male    18-30        1265\n",
       "        30-45         790\n",
       "female  18-30         597\n",
       "        30-45         457\n",
       "male    45-60         154\n",
       "female  45-60         142\n",
       "male    Under 18       61\n",
       "female  Over 60        21\n",
       "        Under 18       17\n",
       "male    Over 60         9\n",
       "other   Under 18        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_demographics[['gender','age_group']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "english_first_language  education   \n",
       "0                       bachelors       1148\n",
       "                        hs               842\n",
       "                        masters          449\n",
       "                        professional     383\n",
       "1                       bachelors        293\n",
       "                        hs               184\n",
       "                        masters           97\n",
       "0                       some              77\n",
       "1                       professional      58\n",
       "0                       doctorate         25\n",
       "1                       some              16\n",
       "                        doctorate         16\n",
       "                        none               2\n",
       "0                       none               1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_demographics[['english_first_language', 'education']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender  age_group  education     english_first_language\n",
       "male    18-30      bachelors     0                         438\n",
       "                   hs            0                         347\n",
       "        30-45      bachelors     0                         226\n",
       "female  18-30      bachelors     0                         215\n",
       "male    30-45      hs            0                         155\n",
       "female  30-45      bachelors     0                         153\n",
       "male    30-45      masters       0                         145\n",
       "female  18-30      hs            0                         132\n",
       "male    30-45      professional  0                         122\n",
       "        18-30      professional  0                         116\n",
       "                   masters       0                         112\n",
       "                   bachelors     1                         106\n",
       "female  30-45      hs            0                          77\n",
       "        18-30      masters       0                          75\n",
       "        30-45      masters       0                          59\n",
       "        18-30      bachelors     1                          56\n",
       "male    30-45      bachelors     1                          54\n",
       "female  30-45      professional  0                          50\n",
       "        18-30      professional  0                          49\n",
       "male    18-30      hs            1                          46\n",
       "        45-60      hs            0                          40\n",
       "female  30-45      bachelors     1                          40\n",
       "male    45-60      bachelors     0                          38\n",
       "female  30-45      hs            1                          37\n",
       "        45-60      bachelors     0                          34\n",
       "male    18-30      masters       1                          30\n",
       "female  18-30      hs            1                          30\n",
       "male    Under 18   hs            0                          30\n",
       "        30-45      hs            1                          27\n",
       "        18-30      some          0                          27\n",
       "female  45-60      hs            0                          27\n",
       "male    45-60      masters       0                          26\n",
       "        30-45      masters       1                          23\n",
       "        45-60      professional  0                          23\n",
       "female  45-60      hs            1                          23\n",
       "male    18-30      professional  1                          23\n",
       "female  45-60      masters       0                          18\n",
       "        30-45      masters       1                          17\n",
       "        45-60      professional  0                          15\n",
       "male    Under 18   bachelors     0                          13\n",
       "        30-45      professional  1                          13\n",
       "                   some          0                          13\n",
       "female  45-60      bachelors     1                          13\n",
       "        18-30      masters       1                          13\n",
       "                   some          0                          11\n",
       "male    45-60      bachelors     1                           9\n",
       "female  Under 18   hs            0                           8\n",
       "        18-30      professional  1                           8\n",
       "male    18-30      doctorate     0                           7\n",
       "        Under 18   some          0                           7\n",
       "        45-60      masters       1                           6\n",
       "female  30-45      some          0                           6\n",
       "                   professional  1                           6\n",
       "male    Under 18   hs            1                           6\n",
       "female  30-45      doctorate     0                           6\n",
       "male    45-60      hs            1                           6\n",
       "        18-30      doctorate     1                           6\n",
       "        30-45      doctorate     1                           6\n",
       "                                 0                           5\n",
       "        18-30      some          1                           5\n",
       "female  Over 60    masters       0                           4\n",
       "male    Over 60    bachelors     0                           4\n",
       "female  Over 60    hs            0                           4\n",
       "        18-30      some          1                           4\n",
       "        45-60      professional  1                           4\n",
       "        30-45      doctorate     1                           3\n",
       "                   some          1                           3\n",
       "        18-30      doctorate     0                           3\n",
       "        Under 18   some          0                           3\n",
       "                   hs            1                           3\n",
       "        45-60      some          0                           3\n",
       "        Over 60    bachelors     0                           3\n",
       "male    Over 60    bachelors     1                           3\n",
       "female  Over 60    professional  0                           3\n",
       "                   some          0                           3\n",
       "        45-60      masters       1                           2\n",
       "male    Under 18   masters       0                           2\n",
       "                   bachelors     1                           2\n",
       "        45-60      some          0                           2\n",
       "                   professional  1                           2\n",
       "        18-30      none          1                           2\n",
       "female  Under 18   bachelors     0                           2\n",
       "        45-60      some          1                           2\n",
       "        Under 18   bachelors     1                           1\n",
       "        45-60      doctorate     0                           1\n",
       "male    30-45      some          1                           1\n",
       "        45-60      doctorate     0                           1\n",
       "female  Over 60    professional  1                           1\n",
       "                   hs            1                           1\n",
       "male    45-60      some          1                           1\n",
       "female  Over 60    doctorate     0                           1\n",
       "male    Over 60    professional  0                           1\n",
       "                                 1                           1\n",
       "female  Over 60    bachelors     1                           1\n",
       "        18-30      doctorate     1                           1\n",
       "male    Under 18   none          0                           1\n",
       "other   Under 18   hs            0                           1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toxic_demographics[['gender','age_group', 'education', 'english_first_language']].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations of demographic analysis\n",
    "- Gender: \n",
    "  - 64.8% are male\n",
    "  - 35.2% are female\n",
    "- English as the first language:\n",
    "  - 81.5% no\n",
    "  - 18.5% yes\n",
    "- Age group\n",
    "  - 51.9% 18-30 years old\n",
    "  - 34.7% 30-45 years old\n",
    "- Education\n",
    "  - 40.1% bachelors\n",
    "  - 28.6% high school\n",
    "  \n",
    "The most dominant group is male with bachelor's degree, aged 18-30, and whose english is not the first language (12%).  <br> \n",
    "The second dominant group is male with high school degree, aged 18-30, and whose english is not primary language (9%).\n",
    "\n",
    "In order to check if the distribution is representative of the population, I compare these ratios with the demographic data of Wikipedia editors from [\"Wikipedia:Who writes Wikipedia?\"](https://en.wikipedia.org/wiki/Wikipedia:Who_writes_Wikipedia%3F#:~:text=28%25%20editors%20are%20aged%2040%2B.&text=59%25%20of%20the%20editors%20are%20aged%2017%20to%2040.&text=The%20English%20Wikipedia%20currently%20has,contributors%20participate%20in%20community%20discussions). The statistics shows that as of 2008, 84% of English Wikipedia editors were male. Provided that the comments data is from 2001 to 2016, the 64.8% reflects the gender gap between men and women editors. <br>\n",
    "For age, the stats shows that 40% of the editors are from age 18-30, less than the ratio from the dataset. There is a possibility that the data has over-representation of people from age 18-30. <br>\n",
    "The source does not provide enough information regards to the education background and speaking language for comparison. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implications of demographic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although the toxicity demographic dataset might be representative of the demographic profile of wikipedia editors, these ratios are different from the distribution of global internet users or global population. \n",
    "- The model is overly affected by the dominant groups and under affected by minority groups.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Discuss further implications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which, if any, of these demo applications would you expect the Perspective API to perform poorly in? Why?\n",
    "  - I think that most of these demo appplications, such as toxicity timeline or comment slider, will provide a biased result of hostile speech. As implied by the first analysis, some comments can be falsely classified as toxic while in fact they are not. On the other hand, my second analysis suggests that the model is trained to be more sensitive to certain demographic groups and might miss subtile hostile speech by other minority groups. \n",
    "  \n",
    "- What are some kinds of hostile speech that would be difficult to accurately detect using the approach used to train the Perspective API models? \n",
    "  - Speech that does not use any key indicator hostile words and instead use more subtle words, but the general tone is toxic. For example, \"we believe that women naturally have a relatively weak sense of direction and they should not drive on the road.\" \n",
    "  \n",
    "- Imagine you are one of the Google data scientists who maintains the Perspective API. If you wanted to improve the model or the API itself to make it work better for any of these purposes, how should they go about doing that?\n",
    "  - I would try to make the distribution of crowdsourcing workers more representative of the general internect users population. \n",
    "  - I would use a LSTM model rather than a logistic model, since the context of words are essential.\n",
    "  - I would source comments and posts not just from online discussion forums but also from social media like Facebook or Twitter. They provide a large amount of information about online discussions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
